# frame_compare.py
"""
CODE GENERATED BY CHAT GPT 5
Frame comparison utilities: combined comparator (phash, whash, SSIM, ORB),
adaptive threshold computation, scene-gap detection, and debug image export.

Dependencies:
  - pillow
  - imagehash
  - opencv-python (cv2)
  - scikit-image (optional, improves SSIM)
Install example:
  pip install Pillow ImageHash opencv-python scikit-image
"""

from PIL import Image
import imagehash
import os
import statistics
from typing import List, Optional, Tuple, Dict

# optional SSIM
try:
    from skimage.metrics import structural_similarity as ssim
    import numpy as np
    HAVE_SSIM = True
except Exception:
    HAVE_SSIM = False

# optional ORB via OpenCV
try:
    import cv2
    HAVE_CV2 = True
except Exception:
    HAVE_CV2 = False

DEFAULT_HASH_SIZE = 8


def _open_gray_resize(path: str, resize_to: Tuple[int, int] = (320, 180)) -> Optional[Image.Image]:
    try:
        img = Image.open(path).convert("L")
        img = img.resize(resize_to, Image.BICUBIC)
        return img
    except Exception:
        return None


def phash_from_path(path: str, hash_size: int = DEFAULT_HASH_SIZE, thumb: Tuple[int,int]=(320,180)):
    """Return imagehash.ImageHash or None."""
    try:
        img = _open_gray_resize(path, thumb)
        if img is None:
            return None
        return imagehash.phash(img, hash_size=hash_size)
    except Exception:
        return None


def whash_from_path(path: str, hash_size: int = DEFAULT_HASH_SIZE, thumb: Tuple[int,int]=(320,180)):
    try:
        img = _open_gray_resize(path, thumb)
        if img is None:
            return None
        return imagehash.whash(img, hash_size=hash_size)
    except Exception:
        return None


def ssim_score_paths(pa: str, pb: str, thumb=(320, 180)) -> Optional[float]:
    if not HAVE_SSIM:
        return None
    try:
        ia = _open_gray_resize(pa, thumb)
        ib = _open_gray_resize(pb, thumb)
        if ia is None or ib is None:
            return None
        a = np.array(ia).astype(np.float32) / 255.0
        b = np.array(ib).astype(np.float32) / 255.0
        score = ssim(a, b, data_range=1.0)
        return float(score)
    except Exception:
        return None


def orb_match_ratio(pa: str, pb: str, resize=(320, 240)) -> Optional[float]:
    """Return ratio of good matches relative to min keypoints count. None if cv2 missing."""
    if not HAVE_CV2:
        return None
    try:
        a = cv2.imread(pa, cv2.IMREAD_GRAYSCALE)
        b = cv2.imread(pb, cv2.IMREAD_GRAYSCALE)
        if a is None or b is None:
            return None
        ah = cv2.resize(a, resize)
        bh = cv2.resize(b, resize)
        orb = cv2.ORB_create(500)
        kp1, des1 = orb.detectAndCompute(ah, None)
        kp2, des2 = orb.detectAndCompute(bh, None)
        if des1 is None or des2 is None or len(kp1) == 0 or len(kp2) == 0:
            return 0.0
        bf = cv2.BFMatcher(cv2.NORM_HAMMING)
        matches = bf.knnMatch(des1, des2, k=2)
        good = 0
        for m_n in matches:
            if len(m_n) < 2:
                continue
            m, n = m_n
            if m.distance < 0.75 * n.distance:
                good += 1
        denom = max(1, min(len(kp1), len(kp2)))
        return float(good) / float(denom)
    except Exception:
        return None


def compare_frames_combined(pa: str, pb: str, hash_size: int = DEFAULT_HASH_SIZE,
                            thumb=(320, 180)) -> Dict:
    """
    Compare two frames with a combined strategy.
    Returns dict with:
      - 'match' : bool (True = same)
      - metrics: phash_dist, whash_dist, ssim, orb_ratio
    Decision is automatic and adaptive:
      - if SSIM available and >= 0.95 => same
      - else if phash_dist <= phash_thresh AND whash_dist <= phash_thresh => same
      - else if orb_ratio >= 0.35 and ssim >= 0.90 => same
      - else different
    phash_thresh is derived from hash_size (looser for small hashes).
    """
    # compute hashes
    ph_a = phash_from_path(pa, hash_size=hash_size, thumb=thumb)
    ph_b = phash_from_path(pb, hash_size=hash_size, thumb=thumb)
    wh_a = whash_from_path(pa, hash_size=hash_size, thumb=thumb)
    wh_b = whash_from_path(pb, hash_size=hash_size, thumb=thumb)
    ph_dist = 999 if ph_a is None or ph_b is None else int(ph_a - ph_b)
    wh_dist = 999 if wh_a is None or wh_b is None else int(wh_a - wh_b)
    ssim_v = ssim_score_paths(pa, pb, thumb=thumb) if HAVE_SSIM else None
    orb_v = orb_match_ratio(pa, pb) if HAVE_CV2 else None

    # derive adaptive phash threshold baseline
    phash_thresh = max(6, min(24, int(round(hash_size * 1.5 + 6))))  # heuristic baseline

    # decision rules
    match = False
    if ssim_v is not None and ssim_v >= 0.95:
        match = True
    elif ph_dist <= phash_thresh and wh_dist <= phash_thresh:
        match = True
    elif orb_v is not None and orb_v >= 0.35 and (ssim_v is None or ssim_v >= 0.90):
        match = True

    return {"match": match, "ph": ph_dist, "wh": wh_dist, "ssim": ssim_v, "orb": orb_v, "ph_thresh": phash_thresh}


# ------- sequence utilities -------

def phash_sequence_from_dir(directory: str, ext=('jpg', 'jpeg', 'png'), hash_size: int = DEFAULT_HASH_SIZE, thumb=(320,180)):
    files = sorted([f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f)) and f.lower().split('.')[-1] in ext])
    res = []
    for f in files:
        path = os.path.join(directory, f)
        res.append(phash_from_path(path, hash_size=hash_size, thumb=thumb))
    return res


def adj_frame_hash_diffs(hashes) -> List[int]:
    diffs = []
    prev = None
    for h in hashes:
        if prev is not None and prev is not None and h is not None:
            try:
                diffs.append(int(h - prev))
            except Exception:
                diffs.append(0)
        elif prev is not None:
            diffs.append(0)
        prev = h
    return diffs


def adaptive_phash_threshold_from_sequences(hA, hB):
    diffs = adj_frame_hash_diffs(hA) + adj_frame_hash_diffs(hB)
    if not diffs:
        return 12
    m = statistics.mean(diffs)
    s = statistics.pstdev(diffs) if len(diffs) > 1 else 0.0
    thresh = int(round(m + 2.0 * s))
    thresh = max(6, min(24, thresh))
    return thresh


def best_shift_count_matches(hA, hB, max_shift, phash_thresh):
    n = len(hA); m = len(hB)
    best_shift = 0; best_count = -1
    for shift in range(-max_shift, max_shift + 1):
        if shift >= 0:
            a0 = 0; b0 = shift
        else:
            a0 = -shift; b0 = 0
        overlap = min(n - a0, m - b0)
        if overlap <= 0:
            continue
        cnt = 0
        for i in range(overlap):
            ha = hA[a0 + i]; hb = hB[b0 + i]
            if ha is None or hb is None:
                continue
            try:
                if int(ha - hb) <= phash_thresh:
                    cnt += 1
            except Exception:
                pass
        if cnt > best_count:
            best_count = cnt; best_shift = shift
    return best_shift, best_count


def aligned_same_array(hA, hB, shift, phash_thresh):
    n = len(hA); m = len(hB)
    if shift >= 0:
        a0 = 0; b0 = shift
    else:
        a0 = -shift; b0 = 0
    overlap = min(n - a0, m - b0)
    same = []
    for i in range(overlap):
        ha = hA[a0 + i]; hb = hB[b0 + i]
        if ha is None or hb is None:
            same.append(False)
        else:
            try:
                same.append(int(ha - hb) <= phash_thresh)
            except Exception:
                same.append(False)
    return same


def find_contiguous_true_blocks(binary: List[bool], min_len: int) -> List[Tuple[int,int]]:
    blocks = []
    s = None
    for i, v in enumerate(binary):
        if v:
            if s is None:
                s = i
        else:
            if s is not None:
                if i - s >= min_len:
                    blocks.append((s, i - 1))
                s = None
    if s is not None:
        if len(binary) - s >= min_len:
            blocks.append((s, len(binary) - 1))
    return blocks


def find_gap_from_same_array(same: List[bool], required_before_blocks: int, required_after_blocks: int, min_block_len: int):
    blocks = find_contiguous_true_blocks(same, min_block_len)
    if not blocks:
        return None
    for i in range(len(blocks)):
        for j in range(i+1, len(blocks)):
            before_count = i + 1
            after_count = len(blocks) - j
            if before_count >= required_before_blocks and after_count >= required_after_blocks:
                gap_start = blocks[i][1] + 1
                gap_end = blocks[j][0] - 1
                if gap_start <= gap_end:
                    return (gap_start, gap_end)
    # fallback - longest false run with sufficient surrounding trues
    longest = (0, -1)
    s = None
    for idx, val in enumerate(same):
        if not val:
            if s is None:
                s = idx
        else:
            if s is not None:
                if idx - s > longest[1] - longest[0] + 1:
                    longest = (s, idx - 1)
                s = None
    if s is not None:
        if len(same) - s > longest[1] - longest[0] + 1:
            longest = (s, len(same) - 1)
    if longest[1] >= longest[0]:
        before_true = sum(1 for k in range(0, longest[0]) if same[k])
        after_true = sum(1 for k in range(longest[1]+1, len(same)) if same[k])
        if before_true >= required_before_blocks and after_true >= required_after_blocks:
            return longest
    return None


def save_debug_images_for_gap(dst_dir: str, dirA: str, dirB: str, gap_idx_tuple: Tuple[int,int], before=5, after=5):
    """
    Save up to `before` images before gap start and `after` images after gap end from both dirA (main) and dirB (cut)
    into dst_dir for visual debugging. Filenames:
      debug_main_<gapstart>_<i>.jpg and debug_cut_<gapstart>_<i>.jpg
    """
    os.makedirs(dst_dir, exist_ok=True)
    a_files = sorted([f for f in os.listdir(dirA) if os.path.isfile(os.path.join(dirA, f))])
    b_files = sorted([f for f in os.listdir(dirB) if os.path.isfile(os.path.join(dirB, f))])
    s, e = gap_idx_tuple
    # gather indices to save (clamp)
    indices = list(range(max(0, s - before), s)) + list(range(s, min(len(a_files), e + 1))) + list(range(e + 1, min(len(a_files), e + 1 + after)))
    # trim to available
    # Save from both A and B if exist matching indices; if B is shifted, user should inspect
    saved = 0
    for idx in indices:
        if idx < len(a_files):
            src = os.path.join(dirA, a_files[idx])
            dst = os.path.join(dst_dir, f"debug_main_{s}_{idx}.jpg")
            try:
                Image.open(src).save(dst)
            except Exception:
                pass
        if idx < len(b_files):
            srcb = os.path.join(dirB, b_files[idx])
            dstb = os.path.join(dst_dir, f"debug_cut_{s}_{idx}.jpg")
            try:
                Image.open(srcb).save(dstb)
            except Exception:
                pass
        saved += 1
        if saved >= (before + after + 1) * 2:
            break

# frame_compare.py
import os
import tempfile
import shutil
import math
from PIL import Image
import imagehash
import subprocess
from statistics import mean, median

# use tools.launch_cmdExt to run ffmpeg if available in the environment where this module is imported.
import tools

class FrameComparer:
    """
    Helper to compare frames between two video files in a given time window.
    - extracts only necessary frames using ffmpeg
    - builds phash series for both videos
    - tries to find the 'gap' area by looking for common scenes before and after
      (2 common scenes before, 3 after) as requested.
    - thresholds and window sizes are adaptive based on media properties
    """
    def __init__(self, main_file, cut_file, window_start_sec, window_end_sec, fps=25, band_width=10, max_search_frames=60, debug=False):
        self.main_file = main_file
        self.cut_file = cut_file
        self.start = float(window_start_sec)
        self.end = float(window_end_sec)
        self.fps = int(fps)
        self.band_width = int(band_width)
        self.max_search_frames = int(max_search_frames)
        self.debug = debug
        self.tmpdir = tempfile.mkdtemp(prefix="framecmp_")

    def _extract_frames(self, filename, target_prefix, start, end, fps):
        """
        Extract frames from filename between start and end with ffmpeg at the specified fps.
        Returns list of frame filepaths.
        """
        outdir = os.path.join(self.tmpdir, target_prefix)
        os.makedirs(outdir, exist_ok=True)
        # use fps and -vsync 0 and -q:v for jpeg
        # frame output pattern:
        pattern = os.path.join(outdir, "f_%06d.jpg")
        ffmpeg = tools.software["ffmpeg"]
        duration = max(0.01, end - start)
        cmd = [ffmpeg, "-y", "-ss", str(start), "-t", str(duration),
               "-i", filename,
               "-vf", f"fps={fps}",
               "-qscale:v", "3",
               "-vsync", "0",
               pattern]
        tools.launch_cmdExt(cmd)
        # collect files in order
        files = sorted([os.path.join(outdir, f) for f in os.listdir(outdir) if f.endswith(".jpg")])
        return files

    def _compute_phash_series(self, frame_files):
        """
        Compute imagehash.phash for each image in frame_files.
        Returns list of (index, phash) where phash is a imagehash.ImageHash
        """
        hashes = []
        for i, f in enumerate(frame_files):
            try:
                img = Image.open(f).convert("RGB")
                ph = imagehash.phash(img)
                hashes.append((i, ph))
            except Exception:
                # skip problematic file
                continue
        return hashes

    def _phash_distance(self, h1, h2):
        return (h1 - h2)

    def find_scene_gap_requirements(self, before_common=2, after_common=3):
        """
        Main routine:
         - extract frames from both files between start,end using fps
         - compute phash series
         - look for pattern: 2 matching frames before gap (aligned), 3 matching after gap
         - returns boundary frames in target coordinates: start_frame,end_frame and times
        """
        # extract frames
        frames_main = self._extract_frames(self.main_file, "main", self.start, self.end, self.fps)
        frames_cut  = self._extract_frames(self.cut_file, "cut",  self.start, self.end, self.fps)

        if not frames_main or not frames_cut:
            # extraction failed
            return None

        ph_main = self._compute_phash_series(frames_main)
        ph_cut  = self._compute_phash_series(frames_cut)

        # adapt phash threshold: compute median hamming distance between aligned windows
        # compute distances for aligned segments (min length)
        min_len = min(len(ph_main), len(ph_cut))
        aligned_dists = []
        for i in range(min_len):
            aligned_dists.append(self._phash_distance(ph_main[i][1], ph_cut[i][1]))
        if aligned_dists:
            base_med = median(aligned_dists)
            base_mean = mean(aligned_dists)
            # threshold: allow small multiple of median; clamp between 2 and 12
            threshold = int(max(2, min(12, base_med + max(1, base_mean * 0.3))))
        else:
            threshold = 8

        # build arrays of just phash for quicker operations
        arr_main = [h for i,h in ph_main]
        arr_cut  = [h for i,h in ph_cut]

        # naive cross-correlation to find alignment shifts with low phash distance
        # For each candidate center in cut, compare surrounding frames to main and seek pattern
        Ncut = len(arr_cut)
        Nmain = len(arr_main)
        # We'll slide a relative offset delta where main[idx] ~ cut[idx+delta] (try offsets within +/- band_width)
        # For each delta between -band..+band, evaluate how many matching frames around central region
        best_candidate = None
        best_score = -1

        # Limit offsets
        max_delta = min(self.band_width, max(1, Ncut))
        for delta in range(-max_delta, max_delta + 1):
            # compute match counts
            match_before = 0
            match_after = 0
            # search for positions where we can find 'before_common' consecutive matches
            # scan indices where both indexes valid
            for i in range(0, Ncut):
                j = i + delta
                if j < 0 or j >= Nmain:
                    continue
                d = self._phash_distance(arr_cut[i], arr_main[j])
                if d <= threshold:
                    # potential match; check consecutive for before_common requirement
                    ok_before = True
                    for k in range(1, before_common):
                        ii = i - k
                        jj = j - k
                        if ii < 0 or jj < 0:
                            ok_before = False
                            break
                        if self._phash_distance(arr_cut[ii], arr_main[jj]) > threshold:
                            ok_before = False
                            break
                    if ok_before:
                        match_before += 1
                    # similarly for after_common:
                    ok_after = True
                    for k in range(1, after_common):
                        ii = i + k
                        jj = j + k
                        if ii >= Ncut or jj >= Nmain:
                            ok_after = False
                            break
                        if self._phash_distance(arr_cut[ii], arr_main[jj]) > threshold:
                            ok_after = False
                            break
                    if ok_after:
                        match_after += 1

            # score: prefer deltas with at least one before match and one after match and with larger sums
            score = match_before + match_after * 2
            if score > best_score:
                best_score = score
                best_candidate = delta

        # if best candidate weak, return None
        if best_candidate is None or best_score <= 0:
            return None

        # Now find precise boundary frames in cut using best_candidate:
        delta = best_candidate
        # compute evidence array for each index in cut: True if distance <= threshold with main at shifted pos
        evidence = []
        for i in range(Ncut):
            j = i + delta
            if 0 <= j < Nmain:
                d = self._phash_distance(arr_cut[i], arr_main[j])
                evidence.append(d <= threshold)
            else:
                evidence.append(False)

        # find first long run of False that separates two runs of True with 2 before and 3 after
        # scan for index where left has before_common trues and right has after_common trues
        for i in range(1, Ncut - 1):
            # test separation point i..i+gaplen
            # find a gap window starting at i of length upto max_search_frames where evidence is mostly False
            for gap_len in range(1, min(self.max_search_frames, Ncut - i)):
                left_ok = all(evidence[max(0, i - before_common):i])
                right_ok = all(evidence[i + gap_len: min(Ncut, i + gap_len + after_common)])
                mid_all_false = all(not e for e in evidence[i:i + gap_len])
                if left_ok and right_ok and mid_all_false:
                    # map frames to times: frame index in cut -> time = start + idx/fps
                    start_frame = i
                    end_frame = i + gap_len - 1
                    start_time = self.start + float(start_frame) / float(self.fps)
                    end_time   = self.start + float(end_frame) / float(self.fps)
                    # return in absolute target coordinates (we assume cut indexing)
                    return {
                        "start_frame": start_frame,
                        "end_frame": end_frame,
                        "start_time": start_time,
                        "end_time": end_time,
                        "delta": delta,
                        "threshold": threshold,
                        "score": best_score
                    }

        # nothing found
        return None

    def cleanup(self):
        try:
            shutil.rmtree(self.tmpdir)
        except Exception:
            pass

    # ensure temp cleanup on delete
    def __del__(self):
        try:
            self.cleanup()
        except Exception:
            pass
